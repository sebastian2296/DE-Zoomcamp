# Workflow Orchestration

## Last Week 
* We started with the definition of a pipeline: a script/job that pulls data from one or more
data sources, makes some transformations on the data and outputs one or more datasets.

* The pipeline implemented on week 1 is actually an example of how not to write a pipeline because it could fail at various stages. It'd be better to split it into two jobs, one isolating
the download/wget of data sources (imagine the internet connection goes down) and the other making transformations and ingesting the data into the database.  

    * When creating this two pipelines we should specify parameters for each one: URL and month for the download pipeline and database access parameters to the second one.

    * How to implement this? two python scripts and a bash script, it's critical to think in ensuring that these scripts are executed in the correct order.

## A slighty more complicated example

* We'll implement a  four-script job (four pipelines) and this will be our workflow structure: 
    * The first script will download the data and store it 
    in csv format locally. 
    * The second script will transform the data from csv format to parquet format. 
    * The third script will upload the data to the datalake (cloud storage)
    * The fourth script takes the parquet data and uploads it to big query.

Note: each step depends on each other for the entire job to be succesful

This series of steps is also known as DAG (Directed Acyclic Graph): edges between nodes are dependencies between each job and is directed one towards the other. Is Acyclic because there are not loops between the any nodes.


## How to actually implement this? Orchestration

* Orchestration ensures that all the jobs are executed in the correct order and hence minimizes
failure.
    * Multiple software options: 
        * Luigi
        * Apache Airflow
        * Prefect

