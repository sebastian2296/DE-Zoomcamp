## Airflow Architecture 

* We'll be using Airflow as an orchestration tool to define and parametrize our workflow, here's an overview of Airflow's architecture:

![alt text](https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/airflow/img/airflow_architecture.png)


* the **scheduler** triggers the scheduled workflows (as specified in the DAGs script) and also submitting tasks to the executor.

* the **exectutor** is where the task is actually ran. In this particular instance, the executor is inside the scheduler. 

* The **worker** executes the task given by the scheduler. 

* A **webserver** that serves as GUI. 

* A **DAG** directory in which the scripts are stored (python files that make use of airflow libraries to specify DAGs)

* A **metadata database** (postgres) used by the scheduler, the executor and the webserver to store logs. Airflow's backend. 

* Other components: 
    * `Redis`: a message broker that forwards messages from scheduler to workers.
    * `Flower`: app for monitoring the environment.
    * `airflow-init`: initialization service which we will customize for our needs.

Airflow will create a folder structure when running: 

* `./dags` - DAG folder for DAG files.
* `./logs` - contains logs from task execution and schedueler.
* `./plugins` - for custom plugins.


Now that we've covered Airflow architecture, we're ready to set up Airflow within a Docker container and upload data to our GCP bucket. 

## Setting up Airflow with Docker

**Pre-requisites**

**1.** Rename your GCP account credentials file to `google_credentials.json` and store it in `$HOME/.google/credentials/`.

**2.** docker-compose should be V2.x and you should asign at least 5gb of RAM to Airflow container (that we are about to build).

**Setup**

1. Create an Airflow subdirectory in your workflow directory.

2. Download the official Docker-compose YAML file for the latest Airflow version.

```bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml
```

    * The official images contains multiple service definitions that we are not going to use,
    that's why we'll be using a simplified YAML file.

3. Set up Airflow user: 

```bash
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

4. The base image we just downloaded does not support interaction with GCP, so we need to customize it so that it works with this cloud provider. For this purpose, we'll use a Dockerfile
to add this customization to our base Airflow image.

    * Some specifications/requirements our Docker file will contain:
        * GCP SDK to communicate with our project.
        * requirements.txt to install python dependencies: `apache-airflow-providers-google` to be able to use the GCP SDK and `pyarrow`, a library to work with parquet files.

5. Alter the `x-airflow-common` service definition inside the docker-compose file.

    * Point the build instruction to our Dockerfile, as in this snippet:

    ```yaml
    build:
        context: .
        dockerfile: ./Dockerfile
    ```

    * Add a volume to point it to the folder where we stored our credentials:
    ```yaml
    ~/.google/credentials/:/.google/credentials:ro
    ```

    * Add `GOOGLE_APPLICATION_CREDENTIALS` `AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT` `GCP_PROJECT_ID` `GCP_GCS_BUCKET` environment variables:

    ```yaml
    GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json
    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: 'google-cloud-platform://?  extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json'
    GCP_PROJECT_ID: '<your_gcp_project_id>'
    GCP_GCS_BUCKET: '<your_bucket_id>'
    ```

# Execution 

1. Build the image, it may take ~15 minutes to complete. 

    ```bash
    docker-compose build
    ```

2. Initialize configurations (you only have to run it once when you finished building the image)

    ```bash
    docker-compose up airflow-init
    ```

3. Run the container

    ```bash
    docker-compose up -d
    ```

## Ingesting data to GCS

Now, that we have our Airflow services set, we will run a more sophisticated/complex script to ingest the ny_taxi data to our Google Cloud Storage bucket.

1. First, we'll use a DAG file that makes use of 3 operators: 
    * 1 BashOperator and 2 PythonOperators (to format the file into parquet and to upload the data to GCP) 
    * A BigQueryCreateExternalTableOperator to ingest data from GCS into BigQuery

2. Run docker-compose up (after running docker-compose up airflow-init)

3. Select the DAG from Airflow's GUI and trigger it.

4. Once the DAG finishes, we can look up in our GCP project for the 'trips_data_all' database with an 'external_table'-
