## Airflow Architecture 

* We'll be using Airflow as an orchestration tool to define and parametrize our workflow, here's an overview of Airflow's architecture:

![alt text](https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_2_data_ingestion/img/airflow_architecture.png)


* the **scheduler** triggers the scheduled workflows (as specified in the DAGs script) and also submitting tasks to the executor.

* the **exectutor** is where the task is actually ran. In this particular instance, the executor is inside the scheduler. 

* The **worker** executes the task given by the scheduler. 

* A **webserver** that serves as GUI. 

* A **DAG** directory in which the scripts are stored (python files that make use of airflow libraries to specify DAGs)

* A **metadata database** (postgres) used by the scheduler, the executor and the webserver to store logs. Airflow's backend. 

* Other components: 
    * ´Redis´: a message broker that forwards messages from scheduler to workers.
    * ´Flower´: app for monitoring the environment.
    * ´airflow-init´: initialization service which we will customize for our needs.

Airflow will create a folder structure when running: 

* ´./dags´ - DAG folder for DAG files.
* ´./logs´ - contains logs from task execution and schedueler.
* ´./plugins´ - for custom plugins.


Now that we've covered Airflow architecture, we're ready to set up Airflow within a Docker container and upload data to our GCP bucket. 

# Setting up Airflow with Docker

## **Pre-requisites**

**1.** Rename your GCP account credentials file to ´google_credentials.json´ and store it in ´$HOME/.google/credentials/´.

**2.** docker-compose should be V2.x and you should asign at least 5gb of RAM to Airflow container (that we are about to build).
