# Ingesting data to local Postgres with Airflow

We will use a docker-compose YAML file to get our Airflow service up and running. Also, we'll start up our Postgres database service (the one that contains the ny_taxi data) to populate/replace the data, but this time using Airflow as an orchestration tool.

1. The first step will be to prepare an ingestion script. For this purpose, we'll use the ingest.py script and put it in the .dags/ folder.

    * This script is essentially the same we used in week 1, except for the fact that this time is divided in two parts: 1)Download and 2)Populate the database.

    * The first part (download) will be assigned to a DAG (BashOperator) that will retrieve the data from the source.

    * The second part (ingest) will be assigned to a second DAG (PythonOperator, but we could've used a DockerOperator as well) that will used the data retrieved in part 1 to make some adjustments, connect to the local database and populate it.

All the necessary credentials to access the database will be referenced as environment variables in the .env file.

2. Modify the Airflow docker-compose.yaml file to include the environment variables described in the previous step.

3. Start the Airflow services with docker-compose up and look for the default network ('docker network ls') to which the containers belong. It should be something along the lines of 'airlflow_default'. 

4. Modify the Postgres and PGadmin docker-compose.yaml file to specify the network mentioned in the previous step.

5. Open the Airflow GUI by going to 'localhost:8080' in your browser. Trigger the DAG by clicking on the play icon

    * Triggering the task will queue the jobs in the specified order within the DAG file (main python script) and at the specfied date and scheduled intervals (that we also specify in the DAG definition with datetimes and CRON processes intervals)