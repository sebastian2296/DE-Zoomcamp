# General Concepts

## OLAP vs OLTP 

**OLTP**: Online Transactional Processing

* Databases we used in our backend
* Fast but small updates+
* Normalized Databases
* Increases productivity of end users -> Online shopping

**OLAP**: Online Analytical Processing

* Clean data that we use to get insights
* Periodical updates of huge amounts of data
* Denormalized Databases
* Increases productivity of business managers, data analysts and executives -> Data Science
## What is a DataWarehouse

A DataWarehouse Is an OLAP solution used for reporting and data analysis 

Consists of:

* Meta data
* Raw data
* Summary data

It receives data from a staging area which in turn receives data from direct sources (OLTP systems, flat files)

After a data warehouse is consolidated, it can be further divided into data marts (smaller OLAP systems designed to isolate data based on business area, for example)

## BigQuery

Is a cloud-based data warehouse solution. It is a serverless data warehouse, meaning that there are not servers to manage or database software to install (Iaas).

Provides software as well as infrastrcutre including scalability and high-availability.

Built-in features like:
    * Machine learning
    * Geospatial analysis
    * Business intelligence

Maximizes flexibility by separating the compute engine that analyzes your data from your storage.

### Cost

On demand pricing
* For every 1TB data porcessed is $5


Flat rate pricing
* Based on number of pre requested slots

### Partitions and Clustering

**Partitions**
Feature that enables to create partitions of a dataset by grouping by a particular (just one) dataset feature (time-unit column, ingestion time or integer range partitioning).

When using time unit or ingestion time:

    * Daily (default)
    * Hourly
    * Monthly or yearly

The number of partitions is 4000.
This improves query performance and reduces cost because we are processing less data.



**Clustering**

Clustering: we group/colocate a partition based on a particular value of a feature (up to four features) in the dataset. This also improves filter queries and aggregation queries.

You can specify up to four clustering columns that must be top-level, non-repeated columns

**When to use clustering over partioning?**

* When artitioning results in a small amount of data per partition (< 1GB)
* When we need more than 4000 partitions (we exceed partion limit)
* Data is updated frequently and hence partitions are mutating constanly (every few minutes, for example)

**Automatic reclustering**

As data is added to a clustered table, the newly inserted data can be written to blocks that contain key ranges that overlap with the key ranges in previously written blocks.

These overlapping keys weaken the sort property of the table. To maintain performance, BigQuery performs automatic reclustering in the background to restore the sort property of the table.

For partitioned tables, clustering is maintained for data within the scope of each partition.

### Best practices

**Cost reduction**
* Avoid   `SELECT *`
* Price your queries before running them
* Use clustered or partitioned tables
* Use streaming inserts with caution
* Materialize query results in stages

**Query performance**

* Filter on partitioned columns
* Denormalize data
* Use nested or repeated columns
* Use external data sources appropriately
* Reduce data before using join
* Do not treat `With` clauses as prepared statements
* Avoid oversharding tables 
* Organize tables in diminishing order by row size.

### Internals

It's not indispensable to know the inner workings of BigQuery to become proficient and use it. Nevertheless, it would be useful to learn these if you're going to develop a data product. 

There are three basic components that constitute the BigQuery internals:
    * Colossus: cheap storage of columnar format. This separation between computing and storage makes it less costly. But there's a caveat: network is of much more importance because it determines the speed in which operations are completed.  

    * Jupiter: network that provides 1 TB/s of transfer velocity

    * Dremel: Execution engine that splits the original query into smaller queries that operate on their corresponding part of the dataset. This smaller queries are ran in leaf nodes that communicate over Jupiter with Colossus.

### Machine learning in BigQuery

* Target Audience Data analysts or managers
* No need for python or java knowledge
* No need to export data into a different system

**Pricing**

* Free 
    * 10 GB per month of data storage
    * 1 TB per month of queries processed
    * ML create model step: first 10 GB per month are free

** ML in BigQuery**

![alt text](https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_3_data_warehouse/img/ml_bigquery.png)