## Reading CSV files

To demonstrate how to read CSV files using Spark, we will use FHVH data for January 2021:

```bash
!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-01.csv
```

And then:

```python
df = spark.read.option('header', 'true').csv('fhvhv_tripdata_2021-01.csv')
```

If we take a look to the file with `df.head()` we see that spark is not reading the data with a proper format.

That's why we use pandas: this library infers the correct data types with the `.dtypes` method. Later we'll use this to build a custom schema and apply it to our spark dataframe.

Â¿How are we going to do it?

```python
spark.createDataFrame(df_pandas).schema
```

But this schema is not quite right, we could change some data types so that reading our spark dataframe becomes more memory efficient.

At the end we end up with the following schema:

```python
from pyspark.sql import types
```

```python
schema = types.StructType([

    types.StructField('hvfhs_license_num',types.StringType(),True),
    types.StructField('dispatching_base_num',types.StringType(),True),
    types.StructField('pickup_datetime',types.TimestampType(),True),
    types.StructField('dropoff_datetime',types.TimestampType(),True),
    types.StructField('PULocationID',types.IntegerType(),True),
    types.StructField('DOLocationID',types.IntegerType(),True),
    types.StructField('SR_Flag',types.StringType(),True)
]
)
```

**Reading CSV file with proper schema**

```python
df = spark.read.option('header', 'true').schema(schema).csv('fhvhv_tripdata_2021-01.csv')
```

Finally, we have a spark dataframe with the right data types.


## Spark Internals (Intro)

Spark works using executors (computers on which we are going to distribute the workload) that take data from GCS and do some kind of transformation to them.

The way that workload is distributed is by assigning each worker of the cluster a file from GCS. Once a worker has finished processing the file, will continue processing another file in sequential order.

**Imagine** that we have only one file in GCS. In that case only one executor would process the file and the rest would do nothing. That's why we use *Parquet* files.


## Partitions

**Imagine** that we have only one file in GCS. In that case only one executor would process the file and the rest would do nothing. That's why we use *Parquet* files.

The way in which we tell spark in how many "chunks" we want our original file divided into is with the `repartition` method.

```python
df.repartition(24).write.parquet('location')
```

